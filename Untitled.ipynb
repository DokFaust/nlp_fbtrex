{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argentine Election Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook I analyze a Spanish dataset set up during the [Argentine legislative election](https://en.wikipedia.org/wiki/Argentine_legislative_election,_2017) of 2017. \n",
    "This dataset contains the data of 9 facebook bots, crawled over a period of 16 days, following 45 sources.\n",
    "\n",
    "__Note__: If you haven't done it already, go through the set up in the *README* of [this repo](https://github.com/rugantio/nlp_fbtrex/).\n",
    "\n",
    "## Dataset\n",
    "The dataset was prepared by the [__Facebook Tracking Exposed__](https://facebook.tracking.exposed/) project and can be retrieved in a convenient JSON format from the specific GitHub [__repo__](https://github.com/tracking-exposed/experiments-data/tree/master/silver).\n",
    "There are two separate files that we'll try to breakdown:\n",
    "* __fbtrex-data-\\*.json__ - Contains all impressions relative to single users\n",
    "* __semantic-entities.json__ - Contains all available metadata regarding posts\n",
    "\n",
    "The text field of every posts is enclosed in *semantic-entities.json*, while I can use *fbtrex-data-\\*.json* to correlate which user has visualized this content, thus providing an easy way to investigate the Facebook filter bubble.\n",
    "Given a ready working environment, as explained is the *README* of this repo, just go ahead and download the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Download Argentine dataset in a data subdir\n",
    "mkdir data && cd data\n",
    "wget https://github.com/tracking-exposed/experiments-data/raw/master/silver/fbtrex-data-1.json.zip\n",
    "wget https://github.com/tracking-exposed/experiments-data/raw/master/silver/semantic-entities.json.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: This commands are supposed to be executed in a bash environment, not in the notebook itself. The operation may fail due to permissions.\n",
    "\n",
    "Extract the content from the zip archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Extract JSON from zipped archives\n",
    "cd data\n",
    "unzip fbtrex-data-1.json.zip\n",
    "unzip semantic-entities.json.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the dataset in JSON format, we can use the [JSON Python library](https://docs.python.org/3/library/json.html) to decode its content and store it in a Python variable. The variable type depends on the actual content of the provided file, by [default](https://docs.python.org/3/library/json.html#json-to-py-table) a JSON object is decoded to a dict and an arrays to a list. The recommended approach for working with encoded text files, is to use the [codecs Python library](https://docs.python.org/3/library/codecs.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "with codecs.open('data/semantic-entities.json',encoding='utf-8') as data_json:    \n",
    "    data = json.load(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print to stdout the content of the parsed JSON file just use [pprint](https://docs.python.org/3/library/pprint.html), the data pretty printer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to check if the casting was performed correctly before proceding, the resulting decoded type can be inspected with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you are using Spyder IDE you can keep track of variable simply looking at the variable explorer window.\n",
    "\n",
    "So the JSON is now a list. How many entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_len = len(data)\n",
    "print('There are {} total elements to analyze'.format(data_len+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go deeper. We decoded the JSON to a list, but what kind of list is it? What happened to JSON objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(data_len):\n",
    "    print(type(data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, *data* is not a simple list, it's a nested list of dictionaries! Let's print the *dict_keys*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(data_len):\n",
    "    print(data[i].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting: in the provided dataset there are some entities that don't have a *text* field. So let's first take only the elements that have a text field and put them in a new non-nested list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tex = []\n",
    "for i in range(data_len):\n",
    "    if 'text' in data[i]:\n",
    "        tex.append(data[i]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. We now have an actual working list. Again, how many entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tex_len = len(tex)\n",
    "print('There are actually {} text elements to analyze'.format(tex_len+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good enough for now, later we can make a deeper analysis, associating each *text* key with its *id* key and its *time* key to correlate which user visualizes which entity and when.  \n",
    "\n",
    "It's good practice to have a new txt file for every step in NLP processing. So let's create a new txt file populated with the *text keys* of the *tex list*, __one per line__. \n",
    "\n",
    "Since some of the text values are made of more than one paragraphs, we need to substitute linebreaks (newline character) with a space character. Some caution is needed because some paragraphs have a double linebreak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Swap linebreaks with a space\n",
    "for i in range(tex_len):\n",
    "    tex[i] = tex[i].replace('\\n\\n','\\n')\n",
    "    tex[i] = tex[i].replace('\\n',' ')\n",
    "\n",
    "#Create new txt with text keys (one per line)\n",
    "with codecs.open('data/text.txt','w',encoding='utf-8') as text:\n",
    "    for i in range(tex_len):\n",
    "        text.write('%s\\n' % tex[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the file and check that everything was executed as it should you don't need another editor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print the first couple of lines\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as text:    \n",
    "    print(text.readline())\n",
    "    next(text)\n",
    "    print(text.readline())\n",
    "    \n",
    "#Print the first 5000 characters\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as text:    \n",
    "    print(text.read(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is over, we now have a txt ready to feed our NLP modules!\n",
    "## Language processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining tasks have become incredibly easy thanks to [spaCy](http://alpha.spacy.io/), a NLP Python module which provides:\n",
    "* Non-destructive tokenization\n",
    "* Syntax-driven sentence segmentation\n",
    "* Pre-trained word vectors\n",
    "* Part-of-speech tagging\n",
    "* Named entity recognition\n",
    "* Labelled dependency parsing\n",
    "* A built-in visualizer \n",
    "...and much more, all with just one function!\n",
    "\n",
    "SpaCy also provides some already trained [models](https://alpha.spacy.io/models/) which you can use out-of-the-box to process different languages. SpaCy's core is written in pure C (via Cython), it's currently the [fastest](https://alpha.spacy.io/usage/facts-figures) parser available and makes [multithreading](https://explosion.ai/blog/multithreading-with-cython) profitable by virtue of Cython.\n",
    "\n",
    "Follow the *README* of this repo and install the Spanish language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
