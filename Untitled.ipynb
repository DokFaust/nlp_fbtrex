{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Python for Facebook Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this review I present a non-exhaustive list tools for analyzing a text corpus using Python.  \n",
    "\n",
    "### Requirements (tools and libs)\n",
    "* The code uses __python3__, it can be easily backported to python2 if needed, but that's up to you.\n",
    "* This notebook can be viewed in github and easily cloned, although to run the code you will need to install [__jupyter__](https://jupyter.org/).\n",
    "* Standard python scientific stack is needed. You can install it through the [conda](https://conda.io/) package manager, through your package manager distribution, or using __pip__, Python's own package manager.\n",
    "Modules used consist in [__Pandas__](http://pandas.pydata.org/) for data analysis and visualization, [__scikit-learn__](http://scikit-learn.org/) for comparative machine learning.\n",
    "* NLP libraries: mainly only [__spaCy__](https://spacy.io/) and [__gensim__](https://radimrehurek.com/gensim/). In the future maybe try Stanford's [__CoreNLP__](https://stanfordnlp.github.io/CoreNLP/). I will avoid using mainstream [__NLTK__](www.nltk.org) for now.\n",
    "\n",
    "### Datasets\n",
    "The datasets are generously provided by the [__Facebook Tracking Exposed__](https://facebook.tracking.exposed/) and can be easily retrieved in their Github [repo](https://github.com/tracking-exposed/experiments-data/). In this specific notebook we will use the Argentinian Election dataset (in Spanish) which contains two json files crawled by 9 bots over the course of more than two weeks! \n",
    "\n",
    "To learn how to make bots to crawl facebook and build your own dataset, refer to the main fbtrex project [backend](https://github.com/tracking-exposed/facebook) (UPDATE: I'm currently working on fbtrex guide for researchers).\n",
    "\n",
    "### Setting up\n",
    "It is recommended to work with virtual environments. Python provides easy venv creation and management and allowing to keep a clean global environment, while having a bleeding edge development branch in production.\n",
    "##### Virtual Environment\n",
    "Python 3.3+ comes with a module called [__venv__](https://docs.python.org/3/library/venv.html). For applications that require an older version of Python, [virtualenv](https://virtualenv.pypa.io/en/stable/) must be used.\n",
    "__Note__: Installing with pip install has to be done in your console for permission purposes. Actually all bash snippets are not supposed to be executed inside this nb. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Create a nlp virtual environment for efficient sandboxing\n",
    "python -m venv nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this command creates a nlp directory in this nb directory and places a pyvenv.cfg file in it with a home key pointing to the Python installation from which the command was run. It also creates a bin subdirectory containing a copy of the python binary. It also creates an (initially empty) lib/pythonX.Y/site-packages subdirectory, where pip install modules will end up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Activate nlp venv\n",
    "source nlp/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: You don't specifically need to activate an environment; activation just prepends the virtual environment's binary directory to your path, so that \"python\" invokes the virtual environment's Python interpreter and you can run installed scripts without having to use their full path. However, all scripts installed in a virtual environment should be runnable without activating it, and run with the virtual environment's Python automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install deps\n",
    "As said, most of these tools can be installed globally via your linux distribution's package manager. If not, now that you are in the venv you can use pip to install the single modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Install minimal scientific python stack\n",
    "pip install pandas && pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install NLP modules\n",
    "Let's begin this analysis using only __spaCy__ for data analysis. I suggest to try the [__alpha__](http://alpha.spacy.io/) version, which is provided via the [__spacy-nightly__](https://pypi.python.org/pypi/spacy-nightly) module. Basic and API documentation can be found in the alpha spaCy [subdomain](https://alpha.spacy.io/usage/). For the soon-legacy documentation refer to [main domain](https://spacy.io/docs/usage/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Install the alpha spacy2.0 instead of 1.9\n",
    "pip install spacy-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once installed, we can move on to download spacy language models. In this review we will only use the __Spanish__ model, [es_core_web_sm](https://alpha.spacy.io/models/es), for text mining purposes. I will also load the __English__ model [en_core_web_sm](https://alpha.spacy.io/models/en) to show off spaCy's wonderful skills. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Download Spanish and English models\n",
    "spacy download es_core_web_sm\n",
    "spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
