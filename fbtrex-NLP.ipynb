{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argentine Election Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook I analyze a Spanish dataset set up during the [Argentine legislative election](https://en.wikipedia.org/wiki/Argentine_legislative_election,_2017) of 2017. \n",
    "This dataset contains the data of 9 facebook bots, crawled over a period of 16 days, following 45 sources.\n",
    "\n",
    "__Note__: If you haven't done it already, go through the set up in the *README* of [this repo](https://github.com/rugantio/nlp_fbtrex/).\n",
    "\n",
    "### Roadmap\n",
    "Download dataset -> cast JSON to txt -> tokenization -> normalization -> phrase modeling -> topic mining -> burst the bubble -> word2vec algebra & predictive analysis\n",
    "\n",
    "## Dataset\n",
    "The dataset was prepared by the [__Facebook Tracking Exposed__](https://facebook.tracking.exposed/) project and can be retrieved in a convenient JSON format from the specific GitHub [__repo__](https://github.com/tracking-exposed/experiments-data/tree/master/silver).\n",
    "There are two separate files that we'll try to breakdown:\n",
    "* __fbtrex-data-\\*.json__ - Contains all impressions relative to single users\n",
    "* __semantic-entities.json__ - Contains all available metadata regarding posts\n",
    "\n",
    "The text field of every posts is enclosed in *semantic-entities.json*, while I can use *fbtrex-data-\\*.json* to correlate which user has visualized this content, thus providing an easy way to investigate the Facebook filter bubble.\n",
    "Given a ready working environment, as explained is the *README* of this repo, just go ahead and download the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#Download Argentine dataset in a data subdir\n",
    "\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "#Insert the archive password here!\n",
    "PWD = ''\n",
    "\n",
    "def datacollector(url):\n",
    "    os.system('mkdir data')\n",
    "    filename = os.path.basename(urlparse(url).path)\n",
    "    (filename, header) = urlretrieve(url,filrename)\n",
    "    commandstr = '7z e -y -odata -p'+ PWD + ' ' + filename\n",
    "    os.system(commandstr)\n",
    "    os.system('rm ' + filename)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: This block will probably not work for you, as actually both the zipped datasets are encrypted.\n",
    "If you are willing to make your own analysis, please let us know, it is possible that the whole dataset will be provided upon request.\n",
    "Also you will need 7zip on your machine, plus irllib in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Then we apply this wonderul function WHICH HAS BEEN MADE JUST \n",
    "#FOR THE PORPOUSE OF NOT WORKING AT ALL on both files\n",
    "datacollector('https://github.com/tracking-exposed/experiments-data/raw/master/silver/fbtrex-data-2.json.zip')\n",
    "datacollector('https://github.com/tracking-exposed/experiments-data/raw/master/silver/semantic-entities.json.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: To try out this notebook I made a shorter version of the JSON, I highly recommend to do the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have the dataset in JSON format, we can use the [JSON Python library](https://docs.python.org/3/library/json.html) to decode its content and store it in a Python variable. The variable type depends on the actual content of the provided file, by [default](https://docs.python.org/3/library/json.html#json-to-py-table) a JSON object is decoded to a dict and an arrays to a list. The recommended approach for working with encoded text files, is to use the [codecs Python library](https://docs.python.org/3/library/codecs.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "with codecs.open('data/semantic-entities.json',encoding='utf-8') as data_json:    \n",
    "    data = json.load(data_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To print to stdout the content of the parsed JSON file just use [pprint](https://docs.python.org/3/library/pprint.html), the data pretty printer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's useful to check if the casting was performed correctly before proceding, the resulting decoded type can be inspected with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: If you are using Spyder IDE you can keep track of variable simply looking at the variable explorer window.\n",
    "\n",
    "So the JSON is now a list. How many entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are {} total elements to analyze.'.format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go deeper. We decoded the JSON to a list, but what kind of list is it? What happened to JSON objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    print(type(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, *data* is not a simple list, it's a nested list of dictionaries! Let's print the *dict_keys*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in data:\n",
    "    print(item.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting: in the provided dataset there are some entities that don't have a *text* field. So let's first take only the elements that have a text field and put them in a new non-nested list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tex = [item['text'] for item in data if 'text' in item]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better. We now have an actual working list. Again, how many entities do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are actually 10956 text elements to analyze!\n"
     ]
    }
   ],
   "source": [
    "print('There are actually {} text elements to analyze!'.format(len(tex)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good enough for now, later we can make a deeper analysis, associating each *text* key with its *id* key and its *time* key to correlate which user visualizes which entity and when.  \n",
    "\n",
    "It's good practice to have a new txt file for every step in NLP processing. So let's create a new txt file populated with the *text keys* of the *tex list*, __one per line__. \n",
    "\n",
    "Since some of the text values are made of more than one paragraphs, we need to substitute linebreaks (newline character) with a space character. Some caution is needed because some paragraphs have a double linebreak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Swap linebreaks with a space\n",
    "for i in range(len(tex)):\n",
    "    tex[i] = tex[i].replace('\\n\\n','\\n')\n",
    "    tex[i] = tex[i].replace('\\n',' ')\n",
    "\n",
    "#Create new txt with text keys (one per line)\n",
    "with codecs.open('data/text.txt','w',encoding='utf-8') as text:\n",
    "    for i in range(len(tex)):\n",
    "        text.write('%s\\n' % tex[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the file and check that everything was executed as it should you don't need another editor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print the first 3000 characters\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as text:    \n",
    "    print(text.read(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing is over, we now have a txt ready to feed our NLP modules!\n",
    "## Language processing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text mining tasks have become incredibly easy thanks to [spaCy](http://alpha.spacy.io/), a NLP Python module which provides:\n",
    "* Non-destructive tokenization\n",
    "* Syntax-driven sentence segmentation\n",
    "* Pre-trained word vectors\n",
    "* Part-of-speech tagging\n",
    "* Named entity recognition\n",
    "* Labelled dependency parsing\n",
    "* A built-in visualizer \n",
    "\n",
    "...and much more, all with just one function!\n",
    "\n",
    "SpaCy also provides some already trained [models](https://alpha.spacy.io/models/) which you can use out-of-the-box to process different languages. SpaCy's core is written in pure C (via Cython), it's currently the [fastest](https://alpha.spacy.io/usage/facts-figures) parser available and makes [multithreading](https://explosion.ai/blog/multithreading-with-cython) profitable by virtue of Cython.\n",
    "\n",
    "Follow the *README* of this repo and install the Spanish language model. Now import the model, and load spaCy's pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 s, sys: 2.33 s, total: 5.56 s\n",
      "Wall time: 2.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "import spacy\n",
    "\n",
    "#Initialize SpaCy's pipeline\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have a processing pipeline, we can call a *nlp* instance as if it were a function on a string of text. This will produce a [Doc](https://alpha.spacy.io/api/doc) object, a special container that holds all linguistic annotations of the text fed in.\n",
    "\n",
    "Let's first explore how SpaCy processes a single entity, before diving into the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codecs' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Snip single line of text\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as text:\n",
    "    line_txt = text.readline()\n",
    "\n",
    "#Standard way of processing text \n",
    "doc = nlp(line_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks exactly the same! But what happened under the hood? Have a look at how [spaCy's pipeline](https://alpha.spacy.io/usage/processing-pipelines) is made:\n",
    "\n",
    "__Text -> tokenizer -> tagger -> parser -> ner -> Doc__\n",
    "\n",
    "Text analysis is built from bottom-up. The *tokenizer* creates a *Doc* data structure, breaking the text in tokens and storing their metadata in a tensor. The *tagger* takes these tokens (and their context) and uses the information to make predictions of the part-of-speech tags. The *parser* assigns dependency labels between tokens and segments text in sentences. The *ner*, named entity recognizer, detects and labels named entities.\n",
    "### Sentence detection and segmentation\n",
    "Sentences are automatically extracted from each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, sent in enumerate(doc.sents):\n",
    "    print ('Sentence {}:'.format(i + 1),sent,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) tagging and grammar analysis\n",
    "Using [Pandas](http://pandas.pydata.org/), Python Data Analysis library, we can have a clean table visualization.\n",
    "- Text: The original word text.\n",
    "- POS: The simple part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag, with full morphology!\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "token_text = [token.orth_ for token in doc]\n",
    "token_pos = [token.pos_ for token in doc]\n",
    "token_tag = [token.tag_ for token in doc]\n",
    "token_dep = [token.dep_ for token in doc]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text,token_pos,token_tag,token_dep)), columns=['Text', 'POS','Tag','Dep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating the parse tree\n",
    "SpaCy uses the terms *head* and *child* to describe the words connected by a single arc in the dependency tree. The term *dep* is used for the arc label, which describes the type of syntactic relation that connects the child to the head. As with other attributes, the value of *.dep* is a hash value. You can get the string value with *.dep\\_*.\n",
    "- Text: The original token text.\n",
    "- Dep: The syntactic relation connecting child to head.\n",
    "- Head text: The original text of the token head.\n",
    "- Head POS: The part-of-speech tag of the token head.\n",
    "- Children: The immediate syntactic dependents of the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_text = [token.text for token in doc]\n",
    "token_head_pos = [token.head.pos_ for token in doc]\n",
    "token_head_text = [token.head.text for token in doc]\n",
    "token_dep = [token.dep_ for token in doc]\n",
    "token_children = [[child for child in token.children] for token in doc]\n",
    "pd.DataFrame(list(zip(token_text,token_dep,token_head_text,token_head_pos,token_children)), columns=['Text','Dep','Head text','Head POS','Children'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num, ent in enumerate(doc.ents):\n",
    "    print ('Entity {}:'.format(num + 1),ent,'-', ent.label_,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with displaCy\n",
    "SpaCy has an integrated visualization library that can display the content in two styles: *dep* and *ent*.\n",
    "The *dep* style shows the dependency between words using arcs, the *ent* style prints out the text with colored NER labels wrapped around words.\n",
    "\n",
    "The method *.serve()* launches a local web server for visualization while the method *.render()* generates an image.\n",
    "\n",
    "__Note__: Style *dep* is not working well in Spanish because *tag* is used instead of *POS* for annotating words, but the *tag* field is much larger than *POS* thus causing overlapping. \n",
    "\n",
    "__Note2__: Style *ent* can't be viewed in Github, but in Jupyter is great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "#displacy.serve(doc, style='dep')\n",
    "options = {'distance':425, 'arrow_spacing':6}\n",
    "displacy.render(doc,style='dep', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#displacy.serve(doc,style='dep')\n",
    "displacy.render(doc,style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text normalization: stemming, lemmatization and shape analysis\n",
    "Let's now move on to single token analysis. *Normalization* is a way of processing text that involves changing the words to make them less unique. We talk about *stemming* when we take the words and we remove the end part, producing a new token that often is not in the language dictionary. *Lemmatization* takes inflected words as input and tries to give the root word as output, so in some way is similar to stemming, but it produces meaningful (actually existing) words. The token *shape* is the de-capitalization char mask that gets applied to the original (orthodox) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_lemma = [token.lemma_ for token in doc]\n",
    "token_shape = [token.shape_ for token in doc]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too bad, lemmatization is actually not supported for the Spanish language model (for the English model however it has good support). We still have some normalization, as seen from the shape mask applied to every word.\n",
    "\n",
    "### Token-level entity analysis\n",
    "The standard way to access entity annotations is the *doc.ents* property, but you can also access token entity annotations using the *token.ent_iob* and *token.ent_type* attributes; *token.ent_iob* indicates whether an entity starts, continues or ends on the tag.\n",
    "\n",
    "IOB Scheme:\n",
    "- *I* - Token is *inside* an entity.\n",
    "- *O* - Token is *outside* an entity.\n",
    "- *B* - Token is the *beginning* of an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_entity_type = [token.ent_type_ for token in doc]\n",
    "token_entity_iob = [token.ent_iob_ for token in doc]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)), columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token-level attributes\n",
    "Other useful metadata is provided, such as the relative frequency of tokens, and whether or not a token matches any of these categories:\n",
    "- stop-word\n",
    "- punctuation\n",
    "- whitespace\n",
    "- number\n",
    "- url\n",
    "\n",
    "...and many more token [attributes](https://alpha.spacy.io/api/token#attributes)! If you are using the alpha version of spaCy, you can also add [custom attributes](https://explosion.ai/blog/spacy-v2-pipelines-extensions) to tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_attributes = [(token.orth_,token.prob,token.is_stop,token.is_punct,token.is_space,token.like_num,token.like_url) for token in doc]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,columns=['text','log_probability','stop?','punctuation?','whitespace?','number?','url?'])\n",
    "\n",
    "df.loc[:, 'stop?':'url?'] = (df.loc[:, 'stop?':'url?'].applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative frequency is not stored in the model, but that's not important since we don't intend to rely on it anyways.  We can see that there are some problems with stop-words, for example \"trabajo\" should not be considered a stop-word, thus in the next section we have to manually adjust this attribute.\n",
    "\n",
    "## Text normalization, lemmatization, stop-words removal and sentence segmentation\n",
    "Now that we have explored all that spaCy can do for us, we can use it to parse our *text.txt* and generate a new *parsed_text.txt* that has the same text, normalized, lemmatized, deprived of stop-words and segmented in sentences.\n",
    "\n",
    "We first define a helper function that constructs a generator to loop over the *text.txt* and yield the review one-by-one. A generator is similar to an iterator but it can be used only once because its content is generated on the fly and not stored in memory, saving precious computation.\n",
    "\n",
    "Then, we pass on the reviews to spaCy using the *.pipe()* method via a generator function to parse the reviews, lemmatize the text, and yield segmantized sentences. The standard way to initialize spaCy would be to call *nlp(text.txt)* on each review, but I will make use instead of the *.pipe()* method which allows efficient [multi-threading](https://spacy.io/docs/usage/processing-text#multithreading). Two [arguments](https://alpha.spacy.io/api/language#pipe) are given to *.pipe()*: *batch_size* is the number of reviews to buffer and *n_threads* which is the number of worker threads to use (default is 2, if -1 OpenMP will decide how many to use at run time). You can also pass a *disable* option to turn off some components of the pipeline that is not needed to further optimize the processing. Note that all processing algorithms are linear-time in the length of the string. \n",
    "\n",
    "Luckily for us, spaCy makes it really easy to modify the pipeline. As explained in the former section, we are going to insert some custom stop-words that were missing in our vocabulary and remove some other ones. As you can see some tokens should be considered stop-words, such as \"y\" and \"a\" are not correctly identified, and, viceversa, words such as \"trabajo\" should not be classified as stop-words. To fix this, we have to list all stop-words present in our model and if they are not supposed to be stop-words we can manually remove them from the pipeline, while to include new stop-words we just have to see if they appear in our topic models and only then come back and label them as stop-words. Another thing we will add to spaCy's pipeline is a custom normalization that replaces accent characters such as *è* and *é* with regular characters such as a simpler *e*, because some people choose to use accents and some don't, and in topic modeling we don't want to have two separate entries for *macrì* and *macri*. We also fix some punctuation although it will be removed anyway by the lemmatizer.\n",
    "\n",
    "Finally, we write the sentences to a new txt file, *parsed_text.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Helper function that yields all reviews via generator \n",
    "def get_review(filename):\n",
    "    with codecs.open(filename,encoding='utf-8') as textfile:\n",
    "        for review in textfile:\n",
    "            review = review.replace('ó','o')            \n",
    "            review = review.replace('ó','o')\n",
    "            review = review.replace('Ó','o')\n",
    "            review = review.replace('Ò','o')\n",
    "            review = review.replace('í','i')\n",
    "            review = review.replace('ì','i')\n",
    "            review = review.replace('Ì','i')            \n",
    "            review = review.replace('Í','i')            \n",
    "            review = review.replace('à','a')\n",
    "            review = review.replace('á','a')\n",
    "            review = review.replace('À','a')\n",
    "            review = review.replace('Á','a')\n",
    "            review = review.replace('ù','u')\n",
    "            review = review.replace('Ù','u')\n",
    "            review = review.replace('Ú','u')\n",
    "            review = review.replace('ú','u')\n",
    "            review = review.replace('è','e')\n",
    "            review = review.replace('é','e')\n",
    "            review = review.replace('È','e')\n",
    "            review = review.replace('É','e')\n",
    "            review = review.replace('¿','')\n",
    "            review = review.replace('“','\\\"')\n",
    "            review = review.replace('”','\\\"')            \n",
    "            yield review\n",
    "            \n",
    "#Add and remove custom stop words\n",
    "nlp.vocab[\"A\"].is_stop = True\n",
    "nlp.vocab[\"a\"].is_stop = True\n",
    "nlp.vocab[\"Y\"].is_stop = True\n",
    "nlp.vocab[\"y\"].is_stop = True\n",
    "nlp.vocab[\"o\"].is_stop = True\n",
    "nlp.vocab[\"O\"].is_stop = True\n",
    "nlp.vocab[\"the\"].is_stop = True\n",
    "nlp.vocab[\"The\"].is_stop = True\n",
    "nlp.vocab[\"e\"].is_stop = True\n",
    "nlp.vocab[\"E\"].is_stop = True\n",
    "nlp.vocab[\"ciento\"].is_stop = True\n",
    "nlp.vocab[\"año\"].is_stop = True\n",
    "nlp.vocab[\"años\"].is_stop = True\n",
    "nlp.vocab[\"trabajo\"].is_stop = False\n",
    "nlp.vocab[\"Trabajo\"].is_stop = False\n",
    "nlp.vocab[\"Trabajar\"].is_stop = False\n",
    "nlp.vocab[\"trabajan\"].is_stop = False\n",
    "nlp.vocab[\"Trabaja\"].is_stop = False\n",
    "nlp.vocab[\"trabaja\"].is_stop = False\n",
    "nlp.vocab[\"tiempo\"].is_stop = False\n",
    "nlp.vocab[\"Tiempo\"].is_stop = False\n",
    "nlp.vocab[\"Respecto\"].is_stop = False\n",
    "nlp.vocab[\"respecto\"].is_stop = False\n",
    "nlp.vocab[\"primero\"].is_stop = False\n",
    "nlp.vocab[\"primera\"].is_stop = False\n",
    "nlp.vocab[\"PRIMERO\"].is_stop = False\n",
    "nlp.vocab[\"primeros\"].is_stop = False\n",
    "nlp.vocab[\"primer\"].is_stop = False\n",
    "nlp.vocab[\"Primero\"].is_stop = False\n",
    "nlp.vocab[\"Primera\"].is_stop = False\n",
    "nlp.vocab[\"Momento\"].is_stop = False\n",
    "nlp.vocab[\"momento\"].is_stop = False\n",
    "nlp.vocab[\"MOMENTO\"].is_stop = False\n",
    "nlp.vocab[\"Estado\"].is_stop = False\n",
    "nlp.vocab[\"estado\"].is_stop = False\n",
    "nlp.vocab[\"Estados\"].is_stop = False\n",
    "nlp.vocab[\"grandes\"].is_stop = False\n",
    "nlp.vocab[\"diferente\"].is_stop = False\n",
    "nlp.vocab[\"diferentes\"].is_stop = False\n",
    "nlp.vocab[\"realizar\"].is_stop = False\n",
    "nlp.vocab[\"realizado\"].is_stop = False\n",
    "nlp.vocab[\"REALIZAR\"].is_stop = False\n",
    "nlp.vocab[\"proximo\"].is_stop = False\n",
    "nlp.vocab[\"empleo\"].is_stop = False\n",
    "nlp.vocab[\"Empleo\"].is_stop = False\n",
    "nlp.vocab[\"acuerdo\"].is_stop = False\n",
    "nlp.vocab[\"pasado\"].is_stop = False\n",
    "nlp.vocab[\"pasada\"].is_stop = False\n",
    "nlp.vocab[\"Van\"].is_stop = False\n",
    "nlp.vocab[\"finally\"].is_stop = False\n",
    "nlp.vocab[\"General\"].is_stop = False\n",
    "nlp.vocab[\"general\"].is_stop = False\n",
    "nlp.vocab[\"Asi\"].is_stop = False\n",
    "nlp.vocab[\"misma\"].is_stop = False\n",
    "nlp.vocab[\"mismo\"].is_stop = False\n",
    "nlp.vocab[\"mismas\"].is_stop = False\n",
    "nlp.vocab[\"mismos\"].is_stop = False\n",
    "nlp.vocab[\"nuevo\"].is_stop = False\n",
    "nlp.vocab[\"nuevos\"].is_stop = False\n",
    "nlp.vocab[\"Nuevo\"].is_stop = False\n",
    "nlp.vocab[\"NUEVO\"].is_stop = False\n",
    "nlp.vocab[\"nuevas\"].is_stop = False\n",
    "nlp.vocab[\"Nueva\"].is_stop = False\n",
    "nlp.vocab[\"nueva\"].is_stop = False\n",
    "nlp.vocab[\"igual\"].is_stop = False\n",
    "nlp.vocab[\"Igual\"].is_stop = False\n",
    "nlp.vocab[\"Debido\"].is_stop = False\n",
    "nlp.vocab[\"debido\"].is_stop = False\n",
    "nlp.vocab[\"ejemplo\"].is_stop = False\n",
    "nlp.vocab[\"verdad\"].is_stop = False\n",
    "nlp.vocab[\"Verdad\"].is_stop = False\n",
    "nlp.vocab[\"valor\"].is_stop = False\n",
    "nlp.vocab[\"Valor\"].is_stop = False\n",
    "nlp.vocab[\"VALOR\"].is_stop = False\n",
    "nlp.vocab[\"HASTA\"].is_stop = False\n",
    "nlp.vocab[\"hasta\"].is_stop = False\n",
    "nlp.vocab[\"Hasta\"].is_stop = False\n",
    "nlp.vocab[\"Buenos\"].is_stop = False\n",
    "nlp.vocab[\"buenos\"].is_stop = False\n",
    "nlp.vocab[\"BUENOS\"].is_stop = False\n",
    "nlp.vocab[\"medio\"].is_stop = False\n",
    "nlp.vocab[\"Medio\"].is_stop = False\n",
    "nlp.vocab[\"lugar\"].is_stop = False\n",
    "nlp.vocab[\"mejor\"].is_stop = False\n",
    "nlp.vocab[\"buena\"].is_stop = False\n",
    "nlp.vocab[\"BUENA\"].is_stop = False\n",
    "nlp.vocab[\"Bueno\"].is_stop = False\n",
    "nlp.vocab[\"bueno\"].is_stop = False\n",
    "nlp.vocab[\"luego\"].is_stop = False\n",
    "nlp.vocab[\"Luego\"].is_stop = False\n",
    "nlp.vocab[\"mal\"].is_stop = False\n",
    "nlp.vocab[\"poco\"].is_stop = False\n",
    "nlp.vocab[\"Poco\"].is_stop = False\n",
    "nlp.vocab[\"Pocos\"].is_stop = False\n",
    "nlp.vocab[\"pocos\"].is_stop = False\n",
    "nlp.vocab[\"embargo\"].is_stop = False\n",
    "nlp.vocab[\"verdadero\"].is_stop = False\n",
    "nlp.vocab[\"verdadera\"].is_stop = False\n",
    "nlp.vocab[\"posible\"].is_stop = False\n",
    "nlp.vocab[\"intento\"].is_stop = False\n",
    "\n",
    "#List current stop words\n",
    "stop_words = []\n",
    "for parsed_review in nlp.pipe(get_review(\"data/text.txt\"),batch_size=10, n_threads=3):\n",
    "    for sent in parsed_review.sents:\n",
    "        for token in sent:\n",
    "            if token.is_stop:\n",
    "                stop_words.append(token.orth_)\n",
    "print(\"There are {} total stop words.\".format(len(stop_words)))\n",
    "stop_set = set(stop_words)\n",
    "print(\"There are {} unique stop words.\".format(len(stop_set)))\n",
    "print(stop_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to lemmatize our corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Generator function to parse reviews, lemmatize the text, and yield sentences\n",
    "#WARNING: This task is computationally demanding, adjust batch_size and n_threads according to your machine\n",
    "def lemmatize_corpus(filename):\n",
    "    for parsed_review in nlp.pipe(get_review(filename),batch_size=10, n_threads=3):\n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent if not (token.is_punct or token.is_space or token.is_stop or token.like_num or token.like_email or token.like_url)])\n",
    "\n",
    "with codecs.open('data/parsed_text.txt','w',encoding='utf-8') as f:\n",
    "    for sent in lemmatize_corpus('data/text.txt'):\n",
    "        f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an excerpt of what we got out of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open('data/parsed_text.txt',encoding='utf-8') as parsed_text:    \n",
    "    print(parsed_text.read(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling with Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Phrase modeling__ is a form of text manipulation that consists in producing new one-word tokens from two or more token. As we saw in named entity recognition, there are groups of words that represent things that have nothing to do with the single words themselves that make up the group. For example *New York* is supposed to be different in meaning from *New* and *York*. We would like to have these single token words joined together in a single word, with an underscore instead of a space. We then repeat the process (__second-order phrase modeling__) to catch three-word tokens such as *New_York_City*.\n",
    "\n",
    "We will use [__gensim__](https://radimrehurek.com/gensim/index.html), an incredible Python library that implements several unsupervised machine learning algorithms designed for text analysis and also some useful text manipulation classes. To accomplish phrase modeling, gensim provides automatic common phrase detection (multiword expressions) from a stream of sentences. The phrases are identified as __collocations__ (frequently co-occurring tokens). In the built-in [*gensim.models.phrases.Phrases*](https://radimrehurek.com/gensim/models/phrases.html) class there are actually two ways ([formulas](https://radimrehurek.com/gensim/models/phrases.html#id2)) for measuring the co-occurrence of these composite words in the corpus, meaning the __frequency__ these words appear __together__ in sequence, compared to the frequency they appear __alone__. We will use the *default* mode, with a *threshold* set to *170*, just enough to catch *mauricio_macri* as a phrase.\n",
    "\n",
    "Gensim's [gensim.models.word2vec.LineSentence](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence) class provides a convenient iterator for working with other gensim components. It streams the sentences from the disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora.\n",
    "\n",
    "Finally, we write the sentences to a new txt file, *bigram_sents.txt*, and we snip the head to check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import warnings\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#Supress useless warning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    #Creates iterable of sentences\n",
    "    unigram_sents = LineSentence('data/parsed_text.txt')\n",
    "\n",
    "    #Initialize the model with our dataset\n",
    "    bigram_model = Phrases(unigram_sents, threshold=170)\n",
    "\n",
    "    #Save and load trained model in data directory (optional)\n",
    "    bigram_model.save('data/bigram_model')\n",
    "    bigram_model = Phrases.load('data/bigram_model')\n",
    "\n",
    "    #Write processed sentences to the new file \n",
    "    with codecs.open('data/bigram_sents.txt','w',encoding='utf-8') as f:\n",
    "        for sent in unigram_sents:\n",
    "            bigram_sent = u' '.join(bigram_model[sent])\n",
    "            f.write(bigram_sent + '\\n')\n",
    "\n",
    "    #Print the first 3000 characters\n",
    "    with codecs.open('data/bigram_sents.txt',encoding='utf-8') as f:    \n",
    "        print(f.read(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that gensim has picked up some composite __phrases__ that we __expected__ such as *mauricio_macri*, *policia_federal*, *codigo_penal* etc. and some __unexpected__ ones such as *acusado_cometer*, *universitario_anibal* etc. With a correct lemmatization this process is far more accurate because inflections in the words account for a larger number of tokens and the co-occurrence model produces more __false positives__. Still, adjusting the threshold to *170* has somewhat mitigated the false processing, producing a functional text.\n",
    "\n",
    "At this point this step is replicated another time, to join three word tokens (second-order phrase modeling). For example if the first pass has joined words like *new* and *york*, producing *new_york*, with a second pass we would expect to join *new_york* and *city*, getting *new_york_city*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Supress useless warning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    #Creates iterable of sentences from our two-word token dataset\n",
    "    bigram_sents = LineSentence('data/bigram_sents.txt')\n",
    "\n",
    "    #Initialize the model with our dataset\n",
    "    trigram_model = Phrases(bigram_sents, threshold=400)\n",
    "\n",
    "    #Save and load trained model in data directory (optional)\n",
    "    trigram_model.save('data/trigram_model')\n",
    "    trigram_model = Phrases.load('data/trigram_model')\n",
    "    \n",
    "    #Write processed sentences to the new file\n",
    "    with codecs.open('data/trigram_sents.txt','w',encoding='utf-8') as f:\n",
    "        for sent in bigram_sents:\n",
    "            trigram_sentence = u' '.join(trigram_model[sent])\n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually really useful in our case, because in Argentina people usually have either a middle name or two last names, and in these cases we are left with a one-word token! For example you see appearing in our corpus words such as *maría_eugenia_vidal* and *alejandra_gils_carbó*. Notice that I chose a higher threshold, *400*, to reduce the number of false positives.\n",
    "\n",
    "Our corpus is actually ready for topic modeling, but before proceding let's see how the text changed. Let's print the same review before and after text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Before:\")\n",
    "with codecs.open('data/text.txt',encoding='utf-8') as f:\n",
    "    print(f.read(845))\n",
    "print(\"________________________________________________\\nAfter:\")\n",
    "with codecs.open('data/trigram_sents.txt',encoding='utf-8') as f:\n",
    "    print(f.read(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a dictionary with Gensim\n",
    "Now that our text is ready to go we can use it to build a gensim [*dictionary*](https://radimrehurek.com/gensim/corpora/dictionary.html), which, in gensim's jargon, consists in a mapping between *words* and their integer *ids*. Dictionaries created from a corpus can later be pruned according to document frequency (removing (un)common words), save/loaded from disk (via *Dictionary.save()* and *Dictionary.load()* methods), merged with other dictionary (*Dictionary.merge_with()*) etc.\n",
    "\n",
    "Dictionary keys in gensim, like in python, constitute a set, thus contain one instance of every word. There are some words that we are not interested in for topic modeling, such as too common or too uncommon words. We can remove them from our dictionary via the *Dictionary.filter_extremes()* method. After some tokens have been removed via there are gaps in the id series. Calling this *Dictionary.compactify()* method will remove these gaps and reassign integer ids.\n",
    "\n",
    "Then we call the *doc2bow()* function to parse our reviews and yield a bag-of-words set. In this \"casting\" the sequential relationship between words is lost, but the number of occurrences of each word of the review is stored in a [vector](https://radimrehurek.com/gensim/tut1.html). We pass the bow set to the [*MmCorpus.serialize()*](https://radimrehurek.com/gensim/corpora/mmcorpus.html) function that iterates through the document stream corpus and saves the bow representation in a simple [Market Matrix](http://math.nist.gov/MatrixMarket/formats.html) format to the disk. Gensim also supports [other formats](https://radimrehurek.com/gensim/tut1.html) such as [Joachim's *SVMlight* format](http://svmlight.joachims.org/), [Blei's LDA-C](http://www.cs.columbia.edu/~blei/lda-c/) format and [GibbsLDA++](http://gibbslda.sourceforge.net/) format.\n",
    "\n",
    "We then easily load the matrix in a variable calling *MmCorpus()*, we will use this variable for topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "#Set up sentence streaming\n",
    "trigram_reviews = LineSentence('data/trigram_sents.txt')\n",
    "\n",
    "#Learn the dictionary by iterating over all of the reviews\n",
    "trigram_dictionary = Dictionary(trigram_reviews)\n",
    "\n",
    "#Get infos about our dict before filtering\n",
    "print(\"Before filtering:\")\n",
    "print(trigram_dictionary)\n",
    "\n",
    "#Filter out words that appear in less than 6 documents or more than 80% reviews\n",
    "trigram_dictionary.filter_extremes(no_below=6, no_above=0.8)\n",
    "\n",
    "#Get infos about our dict after filtering\n",
    "print(\"\\nAfter filtering\")\n",
    "print(trigram_dictionary,'\\n')\n",
    "\n",
    "#Print tokens after filtering\n",
    "#print(trigram_dictionary.token2id)\n",
    "\n",
    "#Generate new ids \n",
    "trigram_dictionary.compactify()\n",
    "   \n",
    "#Save and load the finished dictionary from in data directory (optional)\n",
    "trigram_dictionary.save('data/trigram_dict.dict')\n",
    "trigram_dictionary = Dictionary.load('data/trigram_dict.dict')\n",
    "\n",
    "#Read the reviews and generate bag-of-words representation\n",
    "corpus = [trigram_dictionary.doc2bow(review) for review in trigram_reviews]\n",
    "\n",
    "#Print review vectors\n",
    "#print(corpus)\n",
    "\n",
    "#Save the bow corpus as a matrix\n",
    "MmCorpus.serialize('data/trigram_bow_corpus_all.mm',corpus)\n",
    "\n",
    "\n",
    "#Load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus('data/trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with Gensim\n",
    "What is a topic model? Why would you want to create? Gensim's creator, Radim Rehurek, gives two reasonable answers:\n",
    "- To bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and (hopefully) more semantic way.\n",
    "- To make the document representation more compact. This both improves efficiency (new representation consumes less resources) and efficacy (marginal data trends are ignored, noise-reduction).\n",
    "\n",
    "As a matter of fact, the main problem with topic modeling (and other NLP task) is that we represent documents (reviews) as vector spaces of tokens, and since the __dimension__ of the document vectors is the number of tokens in the corpus vocabulary, it ends up being very __large__. Furthermore every document contains only a small fraction of all tokens in the vocabulary, thus they also tend to be very __sparse__. What we can do is create a new conceptual layer built in our model. Instead of using tokens directly in documents, we describe everything in term of topics: documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary. \n",
    "\n",
    "Gensim provides different algorithms for training a topic model. Now that we created a corpus of documents represented as a stream of vectors we can treat it with different transformations. We are going to try all the transformations available in gensim, starting from the simpler ones and building on them, because [transformation](https://radimrehurek.com/gensim/tut2.html) can be stacked. In order we will train our corpus using:\n",
    "- [__Tf-idf__](https://radimrehurek.com/gensim/models/tfidfmodel.html) (Term frequency - inverse document frequency)\n",
    "- [__LSI__](https://radimrehurek.com/gensim/models/lsimodel.html) (Latent Semantic Indexing) aka LSA\n",
    "- [__RP__](https://radimrehurek.com/gensim/models/rpmodel.html) (Random Projections)\n",
    "- [__HDP__](https://radimrehurek.com/gensim/models/hdpmodel.html) (Hierarchical Dirichlet Process)\n",
    "- [__LDA__](https://radimrehurek.com/gensim/models/ldamodel.html) (Latent Dirichlet Allocation) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf (Term frequency - inverse document frequency)\n",
    "[__Tf-idf__](https://radimrehurek.com/gensim/models/tfidfmodel.html) is the naivest form of training we can do. The way it works is somewhat similar to what we have already done when we created the bag-of-words, but this time the frequencies computed will be a real number:\n",
    "- __Term frequency__: Counts the number of occurencies (frequency) of each term appearing in the dictionary.\n",
    "- __Inverse document frequency__: Introduces a factor that diminishes the weight of terms that occur very frequently in the corpus and increases the weight of terms that occur rarely.\n",
    "\n",
    "In gensim transformations are standard Python objects, typically initialized by means of a training corpus. In case of tf-idf, the \"training\" consists simply of going through the supplied corpus once and computing document frequencies of all its items, increasing the value of rare tokens. In this particular case, we are transforming the same corpus that we used for training, but this is only incidental. Once the transformation model has been initialized, it can be used on any vectors (provided they come from the same vector space, of course), even if they were not used in the training corpus at all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "#Count the frequencies of all tokens in the corpus (training)\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "#Save and load trained tf-idf corpus\n",
    "tfidf.save('data/trainedcorpus.tfidf_model')\n",
    "tfidf = tfidf.load('data/trainedcorpus.tfidf_model')\n",
    "\n",
    "#Transform our corpus using trained corpus\n",
    "print(tfidf)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see *tf-idf* has correctly parsed our corpus, transforming from a bag-of-words __integer__ frequency representation to a tf-idf __real-valued__ frequency weighted matrix, increasing the frequency of rare tokens.\n",
    "\n",
    "We can now use this new *corpus_tfidf* to train other topic mining algorithms instead of the simpler bag-of-words representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI (Latent Semantic Indexing)\n",
    "[__LSI__](https://radimrehurek.com/gensim/models/lsimodel.html) transforms our corpus from Tf-Idf weighted space into a latent space of a lower dimensionality. The \"latency\" is supposed to represent a hidden connection between words (topics, indeed) and can be set at runtime via the *num_topics* parameter. We also turn *onepass* parameter off to force a multi-pass stochastic algorithm and increase *power_iters* and *extra_samples* that represent the number of power iterations  and an oversampling factor respectively, to improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "\n",
    "#Train our corpus with lsi\n",
    "lsi = LsiModel(corpus_tfidf,id2word=trigram_dictionary,num_topics=15,onepass=False,power_iters=1000,extra_samples=100)\n",
    "\n",
    "#Save and load the finished LDA model from disk\n",
    "lsi.save('data/lsi_model')\n",
    "lsi = LsiModel.load('data/lsi_model')\n",
    "\n",
    "# Accept a user-supplied topic number and print out a formatted list of the top terms\n",
    "def explore_topic(topic_number, topn=25):\n",
    "    print ('{:20} {}'.format('term','frequency') + '\\n')\n",
    "    for term, frequency in lsi.show_topic(topic_number, topn=15):\n",
    "        print ('{:20} {:.3f}'.format(term,round(frequency, 3)))\n",
    "print(trigram_dictionary)\n",
    "explore_topic(topic_number=0)\n",
    "explore_topic(topic_number=1)\n",
    "explore_topic(topic_number=2)\n",
    "explore_topic(topic_number=3)\n",
    "explore_topic(topic_number=4)\n",
    "explore_topic(topic_number=5)\n",
    "explore_topic(topic_number=6)\n",
    "explore_topic(topic_number=7)\n",
    "explore_topic(topic_number=8)\n",
    "explore_topic(topic_number=9)\n",
    "explore_topic(topic_number=10)\n",
    "explore_topic(topic_number=11)\n",
    "explore_topic(topic_number=12)\n",
    "explore_topic(topic_number=13)\n",
    "explore_topic(topic_number=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "#import cPickle as pickle\n",
    "\n",
    "#Supress useless warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # workers => sets the parallelism, and should be set to your number of physical cores minus one\n",
    "    lda = LdaMulticore(trigram_bow_corpus,num_topics=15,id2word=trigram_dictionary,workers=3, passes=300)\n",
    "\n",
    "    #Save and load the finished LDA model from disk\n",
    "    lda.save('data/lda_model_all')\n",
    "    lda = LdaMulticore.load('data/lda_model_all')\n",
    "\n",
    "# Accept a user-supplied topic number and print out a formatted list of the top terms\n",
    "def explore_topic(topic_number, topn=25):\n",
    "    print ('{:20} {}'.format('term','frequency') + '\\n')\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=15):\n",
    "        print ('{:20} {:.3f}'.format(term,round(frequency, 3)))\n",
    "print(trigram_dictionary)\n",
    "explore_topic(topic_number=0)\n",
    "explore_topic(topic_number=1)\n",
    "explore_topic(topic_number=2)\n",
    "explore_topic(topic_number=3)\n",
    "explore_topic(topic_number=4)\n",
    "explore_topic(topic_number=5)\n",
    "explore_topic(topic_number=6)\n",
    "explore_topic(topic_number=7)\n",
    "explore_topic(topic_number=8)\n",
    "explore_topic(topic_number=9)\n",
    "explore_topic(topic_number=10)\n",
    "explore_topic(topic_number=11)\n",
    "explore_topic(topic_number=12)\n",
    "explore_topic(topic_number=13)\n",
    "explore_topic(topic_number=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
